# backend/app/articles/service.py
import time
import asyncio
import httpx
import pysbd
import json
import re
import logging
import uuid
from typing import Optional, AsyncGenerator, List
from sqlalchemy import select, func, delete, and_
from sqlalchemy.orm import selectinload
from sqlalchemy.ext.asyncio import AsyncSession
from fastapi import HTTPException

from ..config import settings
from ..users.models import User
from .models import Article, TextCorrectionHistory, ArticlePrompt, ArticleCategory, ArticleStatus, OperationType
from ..styleguides.models import StyleGuide, TextCorrectionHistoryStyle
from ..styleguides import service as style_guide_service
from .services.openai_correction import call_openai_correction_stream, dump_prompt

logger = logging.getLogger(__name__)

def map_category_to_ai_server(category: ArticleCategory) -> str:
    """ArticleCategory enumì„ AI ì„œë²„ category ë¬¸ìì—´ë¡œ ë³€í™˜"""
    mapping = {
        ArticleCategory.SEO: "headlines",
        ArticleCategory.TITLE: "headlines",
        ArticleCategory.BODY: "articles", 
        ArticleCategory.CAPTION: "captions",
    }
    return mapping.get(category, "articles")

async def get_prompt(db: AsyncSession, *, category: ArticleCategory, override: str | None) -> str:
    # ì¶”ê°€ í”„ë¡¬í”„íŠ¸ê°€ ì‹¤ì œ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
    if override and override.strip():
        logger.info(f"Using override prompt for category {category}")
        logger.debug(f"Override prompt content: {override[:200]}...")  # ì²˜ìŒ 200ìë§Œ ë¡œê·¸
        return override.strip()
    
    # Enumì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
    category_str = category.value if isinstance(category, ArticleCategory) else category
    res = await db.execute(
        select(ArticlePrompt.prompt).where(ArticlePrompt.category == category_str)
    )
    p = res.scalar_one_or_none()
    
    if p:
        logger.info(f"âœ… Loaded prompt from DB for category: {category_str}")
        logger.debug(f"DB prompt length: {len(p)} characters")
        logger.debug(f"DB prompt preview: {p[:300]}...")  # ì²˜ìŒ 300ì ë¯¸ë¦¬ë³´ê¸°
    else:
        logger.warning(f"âš ï¸ No prompt found in DB for category: {category_str}, using default")
    
    # ë¹„ì–´ìˆë‹¤ë©´ ì•„ë˜ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
    default_prompt = f"Correct the following {category_str.lower()} text in English with journalistic style."
    return p or default_prompt

async def translate_to_en(text: str) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ (í”„ë¡œë°”ì´ë”ëŠ” envë¡œ ì„ íƒ)"""
    translated, detected_source, target_lang = await translate_text(
        text, source_lang="KO", target_lang="EN-US"
    )
    return translated, detected_source, target_lang

async def translate_text(
    text: str,
    source_lang: Optional[str] = None,
    target_lang: str = "EN-US",
) -> tuple[str, str, str]:
    """ë²”ìš© í…ìŠ¤íŠ¸ ë²ˆì—­ í•¨ìˆ˜ (translation ëª¨ë“ˆ ìœ„ì„)"""
    from .services.translation import translate_text as _translate
    return await _translate(text=text, source_lang=source_lang, target_lang=target_lang)

def _compose_system_prompt(base_prompt: str) -> str:
    logger.info("ğŸ“ Composing system prompt for AI correction")
    logger.debug(f"System prompt total length: {len(base_prompt)} characters")
    logger.debug(f"System prompt preview: {base_prompt[:500]}...")  # ì²˜ìŒ 500ì ë¯¸ë¦¬ë³´ê¸°
    return base_prompt

async def call_ai_server(text: str, category: ArticleCategory) -> dict:
    """AI ì„œë²„ì˜ analyze API í˜¸ì¶œ (ë‹¨ì¼ í…ìŠ¤íŠ¸)
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        category: ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ (TITLE, BODY, CAPTION)
        
    Returns:
        í‘œì¤€í™”ëœ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„± ìœ ì§€)
        
    Raises:
        Exception: AI ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
    """
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{settings.AI_SERVER_URL}/analyze",
                json={
                    "text": text,  # ìƒˆ APIëŠ” "text" í•„ë“œ ì‚¬ìš©
                    "category": map_category_to_ai_server(category)
                },
                headers={"Content-Type": "application/json"}
            )
            response.raise_for_status()
            raw_response = response.json()
            
            # ìƒˆë¡œìš´ ì‘ë‹µ í˜•ì‹ì„ ê¸°ì¡´ ì½”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            return _convert_single_response(raw_response)
            
    except httpx.TimeoutException:
        logger.error(f"AI Server timeout after 60 seconds")
        raise
    except httpx.HTTPStatusError as e:
        logger.error(f"AI Server HTTP error: {e.response.status_code} - {e.response.text}")
        raise
    except Exception as e:
        logger.error(f"AI Server error: {e}")
        raise

def _convert_single_response(raw_response: dict) -> dict:
    """ìƒˆë¡œìš´ AI ì„œë²„ ì‘ë‹µì„ ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        raw_response: AI ì„œë²„ì˜ ìƒˆë¡œìš´ í˜•ì‹ ì‘ë‹µ
        
    Returns:
        ê¸°ì¡´ ì½”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ì˜ ì‘ë‹µ
    """
    # ì‘ë‹µ ê²€ì¦
    if not isinstance(raw_response, dict):
        logger.error(f"Invalid response type: {type(raw_response)}")
        return {"success": False, "message": "Invalid response format"}
    
    violations = raw_response.get("violations", [])
    confidence = raw_response.get("confidence", 0.0)
    adapter_version = raw_response.get("adapter_version", "unknown")
    
    # violations ê²€ì¦
    if not isinstance(violations, list):
        logger.warning(f"Invalid violations format: {type(violations)}, converting to list")
        violations = []
    
    # ë¹ˆ violations ì²˜ë¦¬
    if not violations:
        logger.info("No violations found in AI server response")
        return {
            "success": True,
            "result": {
                "applicable_rules": [],
                "original_text": raw_response.get("text", ""),
                "category": raw_response.get("category", ""),
                "confidence": confidence,
                "adapter_version": adapter_version
            }
        }
    
    # ì‹ ë¢°ë„ ê²€ì¦ ë° ê²½ê³ 
    try:
        confidence = float(confidence)
        if confidence < 0.7:
            logger.warning(f"Low confidence analysis: {confidence:.2f}")
        elif confidence > 1.0:
            logger.warning(f"Suspicious confidence value: {confidence}")
    except (ValueError, TypeError):
        logger.warning(f"Invalid confidence value: {confidence}, defaulting to 0.0")
        confidence = 0.0
    
    # ëª¨ë¸ ë²„ì „ ë¡œê¹…
    logger.debug(f"AI Server adapter version: {adapter_version}")
    
    # violations í˜•ì‹ ê²€ì¦
    valid_violations = []
    for violation in violations:
        if isinstance(violation, str) and violation.strip():
            valid_violations.append(violation.strip())
        else:
            logger.warning(f"Invalid violation format: {violation}")
    
    # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    converted = {
        "success": True,
        "result": {
            "applicable_rules": valid_violations,
            "original_text": raw_response.get("text", ""),
            "category": raw_response.get("category", ""),
            "confidence": confidence,
            "adapter_version": adapter_version
        }
    }
    
    logger.info(f"Converted response: {len(valid_violations)} violations found (confidence: {confidence:.2f})")
    
    return converted

async def call_ai_server_batch(sentences: List[str], category: ArticleCategory) -> dict:
    """AI ì„œë²„ì˜ analyze-batch API í˜¸ì¶œ (ì—¬ëŸ¬ ë¬¸ì¥ ë°°ì¹˜ ì²˜ë¦¬)
    
    Args:
        sentences: ë¶„ì„í•  ë¬¸ì¥ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        category: ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ (ëª¨ë“  ë¬¸ì¥ì— ë™ì¼í•˜ê²Œ ì ìš©)
        
    Returns:
        AI ì„œë²„ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ (ë³‘í•©ëœ ë¶„ì„ ê²°ê³¼)
        
    Raises:
        Exception: AI ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
    """
    if not sentences:
        logger.warning("Empty sentences list provided to batch API")
        return {"success": False, "message": "No sentences to analyze"}
    
    # ë¹ˆ ë¬¸ì¥ ì œê±°
    valid_sentences = [s.strip() for s in sentences if s.strip()]
    if not valid_sentences:
        logger.warning("No valid sentences after filtering")
        return {"success": False, "message": "No valid sentences to analyze"}
    
    try:
        # Batch API í˜•ì‹ìœ¼ë¡œ ë°ì´í„° êµ¬ì„± (ì˜¬ë°”ë¥¸ í‚¤ ì‚¬ìš©)
        batch_data = {
            "items": [
                {"text": sentence, "category": map_category_to_ai_server(category)}
                for sentence in valid_sentences
            ]
        }
        
        logger.info(f"Sending {len(valid_sentences)} sentences to batch API")
        
        async with httpx.AsyncClient(timeout=120.0) as client:  # ë°°ì¹˜ ì²˜ë¦¬ëŠ” ë” ê¸´ timeout
            response = await client.post(
                f"{settings.AI_SERVER_URL}/batch",
                json=batch_data,
                headers={"Content-Type": "application/json"}
            )
            response.raise_for_status()
            
            batch_response = response.json()
            
            # ë°°ì¹˜ ì‘ë‹µì„ ë‹¨ì¼ ì‘ë‹µ í˜•íƒœë¡œ ë³‘í•©
            return _merge_batch_response(batch_response, valid_sentences)
            
    except httpx.TimeoutException:
        logger.error(f"AI Server batch timeout after 120 seconds")
        raise
    except httpx.HTTPStatusError as e:
        logger.error(f"AI Server batch HTTP error: {e.response.status_code} - {e.response.text}")
        raise
    except Exception as e:
        logger.error(f"AI Server batch error: {e}")
        raise

def _merge_batch_response(batch_response: dict, sentences: List[str]) -> dict:
    """ë°°ì¹˜ API ì‘ë‹µì„ ë‹¨ì¼ ì‘ë‹µ í˜•íƒœë¡œ ë³‘í•© (ìƒˆë¡œìš´ í˜•ì‹ ì§€ì›)
    
    Args:
        batch_response: AI ì„œë²„ì˜ ë°°ì¹˜ ì‘ë‹µ (ìƒˆë¡œìš´ í˜•ì‹)
        sentences: ì›ë³¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë³‘í•©ëœ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ í˜•ì‹)
    """
    # ìƒˆë¡œìš´ í˜•ì‹ì—ì„œëŠ” results ë°°ì—´ì— ì§ì ‘ ì‘ë‹µì´ ë“¤ì–´ìˆìŒ
    results = batch_response.get("results", [])
    
    if not results:
        return {"success": False, "message": "No results in batch response"}
    
    # ëª¨ë“  ë¬¸ì¥ì˜ violations ìˆ˜ì§‘
    all_violations = set()
    confidence_scores = []
    adapter_versions = set()
    
    for i, result in enumerate(results):
        if isinstance(result, dict):
            # ìƒˆë¡œìš´ í˜•ì‹ì—ì„œ violations ì¶”ì¶œ
            violations = result.get("violations", [])
            
            # violations ê²€ì¦
            if isinstance(violations, list):
                # ìœ íš¨í•œ violationë§Œ í•„í„°ë§
                valid_violations = [v for v in violations if isinstance(v, str) and v.strip()]
                all_violations.update(valid_violations)
                
                if len(valid_violations) != len(violations):
                    logger.warning(f"Sentence {i+1}: filtered out {len(violations) - len(valid_violations)} invalid violations")
            else:
                logger.warning(f"Sentence {i+1}: invalid violations format: {type(violations)}")
            
            # í†µê³„ ì •ë³´ ìˆ˜ì§‘
            confidence = result.get("confidence", 0.0)
            try:
                confidence = float(confidence)
                confidence_scores.append(confidence)
                
                # ë‚®ì€ ì‹ ë¢°ë„ ê²½ê³ 
                if confidence < 0.7:
                    logger.warning(f"Sentence {i+1} has low confidence: {confidence:.2f}")
                elif confidence > 1.0:
                    logger.warning(f"Sentence {i+1} has suspicious confidence: {confidence:.2f}")
            except (ValueError, TypeError):
                logger.warning(f"Sentence {i+1}: invalid confidence value: {confidence}")
                confidence_scores.append(0.0)
            
            adapter_version = result.get("adapter_version", "unknown")
            if adapter_version and isinstance(adapter_version, str):
                adapter_versions.add(adapter_version)
        else:
            logger.warning(f"Sentence {i+1}: invalid result format: {type(result)}")
    
    # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
    
    # ì–´ëŒ‘í„° ë²„ì „ ë¡œê¹…
    if adapter_versions:
        logger.debug(f"Batch processing adapter versions: {adapter_versions}")
    
    # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    merged_result = {
        "success": True,
        "result": {
            "applicable_rules": sorted(list(all_violations)),
            "total_sentences": len(sentences),
            "processed_sentences": len(results),
            "avg_confidence": avg_confidence,
            "adapter_versions": list(adapter_versions)
        }
    }
    
    logger.info(f"Merged batch response: {len(all_violations)} unique violations from {len(results)} sentences (avg confidence: {avg_confidence:.2f})")
    
    return merged_result

async def call_openai_stream_native(prompt: str, text: str) -> AsyncGenerator[str, None]:
    """OpenAI Async APIë¥¼ ì‚¬ìš©í•œ ë„¤ì´í‹°ë¸Œ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„"""
    try:
        from openai import AsyncOpenAI
        import time
        
        start_time = time.time()
        collected_text = []
        
        client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        sys_prompt = _compose_system_prompt(prompt)
        
        # OpenAI ìš”ì²­ íŒŒë¼ë¯¸í„° ë¡œê¹…
        openai_input = f"{sys_prompt}\n\nText to correct:\n{text}"
        logger.info(f"ğŸ“¡ OpenAI API: Model={settings.OPENAI_MODEL}, Input={len(openai_input)} chars")
        
        # ìƒì„¸ ì •ë³´ëŠ” ë””ë²„ê·¸ ë ˆë²¨ë¡œ
        logger.debug("="*80)
        logger.debug("ğŸš€ OpenAI API Request Details:")
        if hasattr(settings, 'OPENAI_REASONING_EFFORT') and settings.OPENAI_REASONING_EFFORT:
            logger.debug(f"Reasoning Effort: {settings.OPENAI_REASONING_EFFORT}")
        logger.debug(f"System Prompt Length: {len(sys_prompt)} characters")
        logger.debug(f"Text Length: {len(text)} characters")
        logger.debug("-"*40)
        logger.debug(f"Full Input (first 2000 chars):\n{openai_input[:2000]}...")
        if len(openai_input) > 2000:
            logger.debug(f"... [truncated {len(openai_input) - 2000} more characters]")
        logger.debug("="*80)
        
        # API ìš”ì²­ íŒŒë¼ë¯¸í„° êµ¬ì„±
        request_params = {
            "model": settings.OPENAI_MODEL,
            "input": openai_input,
            "stream": True,
            "temperature": 0.1
        }
        
        # reasoning íŒŒë¼ë¯¸í„°ê°€ í™˜ê²½ë³€ìˆ˜ì— ìˆê³  ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€
        if hasattr(settings, 'OPENAI_REASONING_EFFORT') and settings.OPENAI_REASONING_EFFORT:
            request_params["reasoning"] = {
                "effort": settings.OPENAI_REASONING_EFFORT
            }
            logger.info(f"Using reasoning effort: {settings.OPENAI_REASONING_EFFORT}")
        else:
            logger.info("Reasoning effort not configured, skipping reasoning parameter")
        
        stream = await client.responses.create(**request_params)
        
        event_count = 0
        async for event in stream:
            event_count += 1
            current_time = time.time() - start_time
            
            # Delta ì´ë²¤íŠ¸ ì²˜ë¦¬
            if hasattr(event, 'type') and event.type == 'response.output_text.delta':
                delta_text = getattr(event, 'delta', '') or ""
                
                if delta_text:
                    collected_text.append(delta_text)
                    yield json.dumps({
                        "choices": [{
                            "delta": {"content": delta_text}
                        }]
                    })
                    logger.debug(f"Streamed chunk {event_count} at {current_time:.3f}s")
                    
            elif hasattr(event, 'type') and event.type in ['response.done', 'response.completed']:
                full_text = "".join(collected_text)
                logger.info(f"âœ… OpenAI Response: {current_time:.3f}s, {event_count} events, {len(full_text)} chars")
                
                # ìƒì„¸ ì‘ë‹µì€ ë””ë²„ê·¸ ë ˆë²¨ë¡œ
                logger.debug("-"*40)
                logger.debug(f"Full Response (first 3000 chars):\n{full_text[:3000]}...")
                if len(full_text) > 3000:
                    logger.debug(f"... [truncated {len(full_text) - 3000} more characters]")
                logger.debug("="*80)
                    
    except ImportError:
        logger.error("OpenAI package not installed")
        raise
    except Exception as e:
        logger.error(f"OpenAI streaming error: {e}")
        yield json.dumps({
            "choices": [{
                "delta": {"content": f"Error: {str(e)}"}
            }]
        })

def _generate_correction_prompt(style_guides: List[StyleGuide], text_to_correct: str, additional_prompt: str = None) -> str:
    """
    í† í° ì ˆì•½í˜• êµì • í”„ë¡¬í”„íŠ¸ ìƒì„± (êµì •ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜)
    """
    # í•µì‹¬ ì§€ì‹œë¬¸ (í† í° ì ˆì•½)
    system_instruction = "Fix text using rules below. Output ONLY corrected text, no explanations."
    
    # ê·œì¹™ ì •ë¦¬ (ê°„ê²°í™”)
    rules = []
    for guide in style_guides:
        # JSON ë°°ì—´ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        description = ' '.join(guide.content) if isinstance(guide.content, list) else guide.content
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±° ë° ê°„ì†Œí™”
        clean_description = description.replace('["- ', '').replace('"]', '').replace('\\"', '"')
        rules.append(f"R{guide.number}: {clean_description}")
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìµœì†Œí™”)
    prompt_parts = [
        system_instruction,
        "",
        "RULES:",
        *rules,
        "",
        f"TEXT: {text_to_correct}",
        "",
        "CORRECTED:"
    ]
    
    # ì¶”ê°€ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ê·œì¹™ ì„¹ì…˜ì— ì¶”ê°€
    if additional_prompt and additional_prompt.strip():
        prompt_parts.insert(-3, f"EXTRA: {additional_prompt.strip()}")
        prompt_parts.insert(-3, "")
    
    return "\n".join(prompt_parts)

def _generate_sentence_level_correction_prompt(
    style_guides: List[StyleGuide], 
    sentences: List[str], 
    sentence_violations_map: dict, 
    additional_prompt: str = None
) -> str:
    """
    ë¬¸ì¥ë³„ êµì •ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (JSON í˜•ì‹ ì‘ë‹µ ìš”ì²­)
    """
    # ê·œì¹™ ì •ë¦¬ (ì„¤ëª…ê³¼ ì˜ˆì‹œ í¬í•¨)
    rules = []
    for guide in style_guides:
        # ê·œì¹™ ì„¤ëª…
        description = ' '.join(guide.content) if isinstance(guide.content, list) else guide.content
        clean_description = description.replace('["- ', '').replace('"]', '').replace('\\"', '"')
        
        # ê·œì¹™ í…ìŠ¤íŠ¸ êµ¬ì„±
        rule_text = f"R{guide.number}: {clean_description}"
        
        # ì˜ˆì‹œê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if guide.examples_incorrect and guide.examples_correct:
            incorrect_examples = guide.examples_incorrect if isinstance(guide.examples_incorrect, list) else [guide.examples_incorrect]
            correct_examples = guide.examples_correct if isinstance(guide.examples_correct, list) else [guide.examples_correct]
            
            # ì˜ˆì‹œê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if incorrect_examples and correct_examples:
                rule_text += "\n  Examples:"
                for i, (incorrect, correct) in enumerate(zip(incorrect_examples[:2], correct_examples[:2]), 1):  # ìµœëŒ€ 2ê°œ ì˜ˆì‹œë§Œ
                    if incorrect and correct:
                        rule_text += f"\n    âœ— Incorrect: {incorrect}"
                        rule_text += f"\n    âœ“ Correct: {correct}"
                        if i < min(2, len(incorrect_examples)):  # ë‹¤ìŒ ì˜ˆì‹œê°€ ìˆìœ¼ë©´ êµ¬ë¶„ì„ 
                            rule_text += "\n"
        
        rules.append(rule_text)
    
    # ë¬¸ì¥ë³„ violations ì •ë³´ ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë¬¸ì¥ ê¸¸ì´ ì œí•œ)
    sentence_info = []
    for idx, sentence in enumerate(sentences):
        # ë§¤ìš° ê¸´ ë¬¸ì¥ì€ truncateí•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
        truncated_sentence = sentence[:500] + "..." if len(sentence) > 500 else sentence
        violations = sentence_violations_map.get(idx, {}).get("violations", [])
        if violations:  # violationsì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            violation_rules = [v.split('_')[-1] for v in violations]  # articles_SG013 -> SG013
            sentence_info.append(f"Sentence {idx+1}: Violates rules {', '.join(violation_rules)}")
    
    prompt_parts = [
        "You are a professional text editor specializing in style guide compliance.",
        "Your task is to correct the following text according to the specific style guide rules that were violated.",
        "",
        "Instructions:",
        "1. Apply ALL the style guide rules listed below to correct the text",
        "2. Pay special attention to the sentences that have specific violations noted",
        "3. Return ONLY the corrected text without any JSON formatting, explanations, or metadata",
        "4. Maintain the original paragraph structure and spacing",
        "",
        "STYLE GUIDE RULES TO APPLY:",
        *rules,
        "",
    ]
    
    # ìœ„ë°˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if sentence_info:
        prompt_parts.extend([
            "SPECIFIC VIOLATIONS TO FIX:",
            *sentence_info,
            ""
        ])
    
    prompt_parts.extend([
        "TEXT TO CORRECT:",
        " ".join(sentences),  # ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        ""
    ])
    
    if additional_prompt and additional_prompt.strip():
        prompt_parts.extend([
            f"ADDITIONAL INSTRUCTION: {additional_prompt.strip()}",
            ""
        ])
    
    prompt_parts.append("CORRECTED TEXT:")
    
    return "\n".join(prompt_parts)

def _build_full_text_from_corrections(sentence_corrections: dict, original_sentences: List[str]) -> str:
    """
    ë¬¸ì¥ë³„ êµì • ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì „ì²´ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
    
    Args:
        sentence_corrections: {index: {"original": str, "corrected": str, "violations": list}}
        original_sentences: ì›ë³¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (spacing/paragraph ì •ë³´ ìœ ì§€ìš©)
    
    Returns:
        str: í•©ì³ì§„ ì „ì²´ êµì • í…ìŠ¤íŠ¸
    """
    try:
        # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ ê°„ êµ¬ë¶„ì ì¶”ì¶œ (ê³µë°±, ì¤„ë°”ê¿ˆ ë“±)
        # pysbdëŠ” ë¬¸ì¥ ë¶„ë¦¬ ì‹œ ì›ë³¸ spacingì„ ì œê±°í•˜ë¯€ë¡œ, ì›ë³¸ì—ì„œ íŒ¨í„´ ì¶”ì¶œ í•„ìš”
        
        # ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ êµì •ëœ ë¬¸ì¥ë“¤ ìˆ˜ì§‘
        corrected_sentences = []
        max_index = max(len(original_sentences) - 1, max(sentence_corrections.keys()) if sentence_corrections else 0)
        
        for i in range(max_index + 1):
            if i in sentence_corrections:
                corrected = sentence_corrections[i].get("corrected", "")
                if corrected:
                    corrected_sentences.append(corrected)
                    logger.debug(f"Sentence {i}: Using corrected version")
                else:
                    # correctedê°€ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                    if i < len(original_sentences):
                        corrected_sentences.append(original_sentences[i])
                        logger.warning(f"Sentence {i}: Corrected is empty, using original")
                    else:
                        logger.warning(f"Sentence {i}: Index out of range, skipping")
            else:
                # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ êµì •ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                if i < len(original_sentences):
                    corrected_sentences.append(original_sentences[i])
                    logger.debug(f"Sentence {i}: No correction found, using original")
                else:
                    logger.warning(f"Sentence {i}: No correction and index out of range, skipping")
        
        # ë¬¸ì¥ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²° (ê¸°ë³¸ì ìœ¼ë¡œ ë‹¨ì¼ ê³µë°±)
        full_text = " ".join(corrected_sentences)
        
        logger.info(f"Built full_text from {len(corrected_sentences)} sentences (total length: {len(full_text)} chars)")
        return full_text
        
    except Exception as e:
        logger.error(f"Error building full_text from corrections: {e}")
        # ì—ëŸ¬ ì‹œ ì›ë³¸ ë¬¸ì¥ë“¤ì„ ê·¸ëŒ€ë¡œ ì—°ê²°
        return " ".join(original_sentences)

def _parse_sentence_corrections(openai_response: str, original_sentences: List[str], sentence_violations_map: dict) -> tuple:
    """
    OpenAI ì‘ë‹µì—ì„œ ë¬¸ì¥ë³„ êµì • ì •ë³´ë¥¼ íŒŒì‹±í•˜ê³  ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        openai_response: OpenAIì—ì„œ ë°›ì€ ì „ì²´ ì‘ë‹µ í…ìŠ¤íŠ¸
        original_sentences: ì›ë³¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        sentence_violations_map: ë¬¸ì¥ë³„ violations ì •ë³´
        
    Returns:
        tuple: (sentence_corrections dict, full_text str)
    """
    sentence_corrections = {}
    
    try:
        response_length = len(openai_response)
        logger.info(f"Parsing OpenAI response (length: {response_length} chars)")
        
        # JSON ì‘ë‹µì—ì„œ corrected_sentences ì¶”ì¶œ ì‹œë„
        if "{" in openai_response and "}" in openai_response:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_start = openai_response.find("{")
            json_end = openai_response.rfind("}") + 1
            json_part = openai_response[json_start:json_end]
            
            logger.debug(f"Extracted JSON part (length: {len(json_part)})")
            
            try:
                parsed_response = json.loads(json_part)
                corrected_sentences = parsed_response.get("corrected_sentences", [])
                
                if not corrected_sentences:
                    logger.warning("No corrected_sentences found in JSON response")
                    # Try alternative field names
                    for alt_field in ["sentences", "corrections", "results"]:
                        if alt_field in parsed_response:
                            corrected_sentences = parsed_response[alt_field]
                            logger.info(f"Found corrections in alternative field: {alt_field}")
                            break
                
                logger.info(f"Processing {len(corrected_sentences)} sentence corrections")
                
                for i, correction in enumerate(corrected_sentences):
                    if isinstance(correction, dict):
                        idx = correction.get("index")
                        if idx is None:
                            # Try alternative index field names
                            idx = correction.get("sentence_index", correction.get("id", i))
                        
                        if idx is not None and isinstance(idx, int):
                            original = correction.get("original", correction.get("before", ""))
                            corrected = correction.get("corrected", correction.get("after", ""))
                            
                            if not original and not corrected:
                                logger.warning(f"Both original and corrected text are empty for index {idx}")
                            
                            sentence_corrections[idx] = {
                                "original": original,
                                "corrected": corrected,
                                "violations": sentence_violations_map.get(idx, {}).get("violations", [])
                            }
                            logger.debug(f"Added correction for sentence {idx}: '{original[:50]}...' -> '{corrected[:50]}...'")
                        else:
                            logger.warning(f"Invalid or missing index for correction {i}: {correction}")
                    else:
                        logger.warning(f"Correction {i} is not a dictionary: {type(correction)}")
                            
                logger.info(f"Successfully parsed JSON response with {len(sentence_corrections)} corrections")
                
                # Build full_text from corrections
                full_text = _build_full_text_from_corrections(sentence_corrections, original_sentences)
                logger.info(f"Built full_text from corrections: {len(full_text)} chars")
                
                return sentence_corrections, full_text
                
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse OpenAI response as JSON: {e}")
                logger.debug(f"JSON parse failed for: {json_part[:200]}...")
        else:
            logger.warning("No JSON structure found in OpenAI response, falling back to text parsing")
    
    except Exception as e:
        logger.warning(f"JSON parsing failed: {e}, falling back to text parsing")
    
    # Fallback: í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì‹± ë˜ëŠ” ì›ë³¸ ë¬¸ì¥ ì‚¬ìš©
    logger.info("Using fallback: mapping original sentences to corrected text")
    
    # ì‘ë‹µì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ì„œ ë§¤í•‘ ì‹œë„
    response_lines = [line.strip() for line in openai_response.split('\n') if line.strip()]
    
    for idx, original_sentence in enumerate(original_sentences):
        # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (êµì •ì´ í•„ìš”ì—†ì„ ìˆ˜ë„ ìˆìŒ)
        corrected_sentence = original_sentence
        
        # ì‘ë‹µì—ì„œ í•´ë‹¹ ë¬¸ì¥ê³¼ ìœ ì‚¬í•œ êµì •ëœ ë¬¸ì¥ ì°¾ê¸° ì‹œë„
        if idx < len(response_lines):
            corrected_sentence = response_lines[idx]
        
        sentence_corrections[idx] = {
            "original": sentence_violations_map.get(idx, {}).get("text", original_sentence),
            "corrected": corrected_sentence,
            "violations": sentence_violations_map.get(idx, {}).get("violations", [])
        }
    
    logger.info(f"Fallback parsing completed with {len(sentence_corrections)} corrections")
    
    # Build full_text from fallback corrections
    full_text = _build_full_text_from_corrections(sentence_corrections, original_sentences)
    logger.info(f"Built full_text from fallback corrections: {len(full_text)} chars")
    
    return sentence_corrections, full_text

async def call_ai_correction_stream(
    prompt: str,
    text: str,
    category: ArticleCategory,
    db: AsyncSession
) -> AsyncGenerator[str, None]:
    """
    AI í…ìŠ¤íŠ¸ êµì •ì„ ìœ„í•œ ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸.

    1. í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³  AI ì„œë²„ ë°°ì¹˜ ë¶„ì„ìœ¼ë¡œ ìœ„ë°˜ ê·œì¹™ íƒì§€
    2. ìœ„ë°˜ ê·œì¹™ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ìƒì„¸ ì •ë³´ë¥¼ DBì—ì„œ ì¡°íšŒ
    3. OpenAI ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤ì œ êµì •ì„ ìˆ˜í–‰ (í•„ìš” ì‹œ DeepL ë²ˆì—­ ì •ë³´ í¬í•¨)
    4. ì‹¤íŒ¨ ì‹œ ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ì„ ë˜ëŠ” OpenAI ë‹¨ë… ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í´ë°±
    """
    # 1. AI Server (ë¶„ì„ê¸° ì—­í• )
    if settings.USE_AI_SERVER and settings.AI_SERVER_URL:
        try:
            # Step 1: í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            logger.info("ğŸ¤– Step 1: Segmenting text into sentences...")
            seg = pysbd.Segmenter(language="en", clean=False)
            sentences = seg.segment(text)
            logger.debug(f"Segmented sentences: {sentences}")
            logger.info(f"Segmented into {len(sentences)} sentences")
            
            # Step 2: AI ì„œë²„ì—ì„œ ë°°ì¹˜ ë¶„ì„
            ai_server_start = time.time()
            logger.info("ğŸ¤– Step 2: Analyzing sentences with AI Server (batch)...")
            
            # batch API ì§ì ‘ í˜¸ì¶œí•´ì„œ ì›ë³¸ ì‘ë‹µ ë³´ì¡´
            try:
                async with httpx.AsyncClient(timeout=120.0) as client:
                    valid_sentences = [s.strip() for s in sentences if s.strip()]
                    batch_data = {
                        "items": [
                            {"text": sentence, "category": map_category_to_ai_server(category)}
                            for sentence in valid_sentences
                        ]
                    }
                    
                    response = await client.post(
                        f"{settings.AI_SERVER_URL}/batch",
                        json=batch_data,
                        headers={"Content-Type": "application/json"}
                    )
                    response.raise_for_status()
                    batch_response = response.json()
                    
                    # ë¬¸ì¥ë³„ violations ì •ë³´ ì €ì¥
                    sentence_violations_map = {}
                    all_violations = set()
                    
                    results = batch_response.get("results", [])
                    for idx, result in enumerate(results):
                        if isinstance(result, dict):
                            violations = result.get("violations", [])
                            sentence_violations_map[idx] = {
                                "text": result.get("text", sentences[idx] if idx < len(sentences) else ""),
                                "violations": violations,
                                "confidence": result.get("confidence", 0.0)
                            }
                            all_violations.update(violations)
                    
                    applicable_rules = list(all_violations)
                    
            except Exception as e:
                logger.error(f"Batch API error: {e}")
                raise
                
            ai_server_time = time.time() - ai_server_start
            
            if not applicable_rules:
                logger.info("No applicable style rules found. Returning original text.")
                for char in text:
                    yield json.dumps({"choices": [{"delta": {"content": char}}]})
                return
            
            logger.info(f"âœ… AI Server batch analysis completed in {ai_server_time:.3f}s. Found rules: {applicable_rules}")
            logger.info(f"Processed {len(results)}/{len(sentences)} sentences with violations map: {len(sentence_violations_map)}")

            # Step 3: ì„œë¹„ìŠ¤ ë ˆì´ì–´ë¥¼ í†µí•´ DBì—ì„œ ê·œì¹™ ìƒì„¸ ì •ë³´ ì¡°íšŒ
            db_lookup_start = time.time()
            logger.info("ğŸ¤– Step 3: Fetching style guide details via service...")
            style_guides_details = await style_guide_service.get_guides_by_applicable_rules(
                db, rule_ids=applicable_rules
            )
            db_lookup_time = time.time() - db_lookup_start
            
            if not style_guides_details:
                raise ValueError(f"Could not find DB details for rules {applicable_rules}.")
            
            logger.info(f"âœ… DB lookup completed in {db_lookup_time:.3f}s. Found {len(style_guides_details)} guide(s).")
             
            # Step 4: ë¶„ì„ ê²°ê³¼ JSONì„ ì²« ë²ˆì§¸ë¡œ yield
            logger.info("ğŸ¤– Step 4: Yielding analysis result...")
            analysis_data = {
                "applicable_rules": applicable_rules,
                "style_guide_violations": [
                    {
                        "id": f"{g.category}_SG{g.number:03d}", # íŒŒì‹± ê°€ëŠ¥í•œ ID í˜•ì‹ìœ¼ë¡œ
                        "category": g.category,
                        "number": g.number,
                        "description": ' '.join(g.content) if isinstance(g.content, list) else g.content
                    }
                    for g in style_guides_details
                ],
                "style_ids": [g.id for g in style_guides_details],  # DB primary key IDs for history storage
                "sentence_violations": sentence_violations_map,  # ë¬¸ì¥ë³„ violations ì •ë³´ ì¶”ê°€
                "batch_info": {
                    "total_sentences": len(sentences),
                    "processed_sentences": len(results),
                    "avg_confidence": sum(s.get("confidence", 0.0) for s in sentence_violations_map.values()) / len(sentence_violations_map) if sentence_violations_map else 0.0
                }
            }
            yield json.dumps({"type": "analysis", "data": analysis_data})

            # Step 5: OpenAIì— ë³´ë‚¼ ë¬¸ì¥ë³„ êµì • í”„ë¡¬í”„íŠ¸ ìƒì„±
            logger.info("ğŸ¤– Step 5: Generating sentence-level correction prompt for OpenAI.")
            final_openai_prompt = _generate_sentence_level_correction_prompt(
                style_guides_details, sentences, sentence_violations_map, prompt
            )
            
            # ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ë¡œê¹… (ë””ë²„ê·¸ ë ˆë²¨)
            logger.debug("="*80)
            logger.debug("ğŸ“ Generated Sentence-Level Prompt for OpenAI:")
            logger.debug(f"Prompt Length: {len(final_openai_prompt)} characters")
            logger.debug(f"Number of Sentences: {len(sentences)}")
            logger.debug(f"Number of Style Guides: {len(style_guides_details)}")
            logger.debug("-"*40)
            logger.debug(f"Full Prompt (first 2500 chars):\n{final_openai_prompt[:2500]}...")
            if len(final_openai_prompt) > 2500:
                logger.debug(f"... [truncated {len(final_openai_prompt) - 2500} more characters]")
            logger.debug("="*80)
            
            # OpenAI API ì‹œê°„ ì¸¡ì •
            openai_start = time.time()
            logger.info("ğŸ¤– Starting OpenAI API correction stream (REAL STREAMING)...")

            # Step 5: OpenAI ìŠ¤íŠ¸ë¦¼ì„ ì¦‰ì‹œ ì „ë‹¬í•˜ë©´ì„œ ë™ì‹œì— ìˆ˜ì§‘
            collected_chunks = []
            chunk_count = 0

            # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¤í”„ (ë””ë²„ê¹…/ê²€ì¦ìš©)
            try:
                dump_prompt(
                    "correction_sentence_level",
                    final_openai_prompt,
                    {
                        "category": category.value if hasattr(category, "value") else str(category),
                        "style_guides": [
                            {
                                "id": g.id,
                                "number": g.number,
                                "category": str(getattr(g, "category", ""))
                            }
                            for g in style_guides_details
                        ],
                        "sentences": len(sentences),
                    },
                )
            except Exception:
                pass
            
            async for chunk_str in call_openai_stream_native(final_openai_prompt, ""):
                try:
                    delta_chunk = json.loads(chunk_str)
                    
                    # ì¦‰ì‹œ í”„ë¡ íŠ¸ì—”ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë°
                    yield json.dumps({
                        "type": "delta",
                        "data": delta_chunk
                    })
                    
                    # ë™ì‹œì— ë‚´ìš© ìˆ˜ì§‘ (ë‚˜ì¤‘ ì²˜ë¦¬ë¥¼ ìœ„í•´)
                    choices = delta_chunk.get("choices", [])
                    if choices and "delta" in choices[0] and "content" in choices[0]["delta"]:
                        content = choices[0]["delta"]["content"]
                        collected_chunks.append(content)
                        chunk_count += 1
                        
                except json.JSONDecodeError:
                    pass
            
            openai_time = time.time() - openai_start
            full_openai_response = "".join(collected_chunks).strip()
            
            # ë¡œê¹… ê°„ì†Œí™”
            logger.info(f"âœ… OpenAI streaming completed: {openai_time:.3f}s, {chunk_count} chunks, {len(full_openai_response)} chars")
            
            # Step 6: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì²˜ë¦¬
            # OpenAI ì‘ë‹µì€ êµì •ëœ ì „ì²´ í…ìŠ¤íŠ¸
            final_text = full_openai_response
            
            # êµì •ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì›ë³¸ê³¼ ë§¤í•‘
            corrected_seg = pysbd.Segmenter(language="en", clean=False)
            corrected_sentences = corrected_seg.segment(final_text)
            
            # ë¬¸ì¥ë³„ êµì • ì •ë³´ ì¬êµ¬ì„± (AI ì„œë²„ì˜ violations ì •ë³´ í™œìš©)
            sentence_corrections = {}
            for idx in range(min(len(sentences), len(corrected_sentences))):
                sentence_corrections[idx] = {
                    "original": sentence_violations_map.get(idx, {}).get("text", sentences[idx] if idx < len(sentences) else ""),
                    "corrected": corrected_sentences[idx] if idx < len(corrected_sentences) else "",
                    "violations": sentence_violations_map.get(idx, {}).get("violations", [])
                }
            
            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ë©”íƒ€ë°ì´í„° ì „ì†¡
            logger.info(f"Parsed {len(sentence_corrections)} sentence corrections")
            
            # ë¬¸ì¥ë³„ êµì • ì •ë³´ ì „ì†¡
            yield json.dumps({
                "type": "sentence_corrections", 
                "data": {
                    "sentence_corrections": sentence_corrections,
                    "total_sentences": len(sentences),
                    "corrected_sentences": len(sentence_corrections),
                    "full_text": final_text or full_openai_response  # íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ì›ë³¸
                }
            })
            
            # Step 7: ìµœì¢… ë¶„ì„ ê²°ê³¼ ìƒì„± (DB ì €ì¥ìš© ë°ì´í„° í¬í•¨)
            final_analysis_result = analysis_data.copy()
            final_analysis_result["sentence_corrections"] = sentence_corrections
            final_analysis_result["full_text"] = final_text or full_openai_response
            
            yield json.dumps({"type": "final_analysis", "data": final_analysis_result})
            
            
            return

        except Exception as e:
            logger.warning(f"AI Server batch flow failed: {e}. Trying single sentence fallback...")
            
            # Fallback: ë‹¨ì¼ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
            try:
                logger.info("ğŸ¤– Fallback: Using single text analysis...")
                analysis_response = await call_ai_server(text, category)
                
                if analysis_response.get("success"):
                    result = analysis_response.get("result", {})
                    applicable_rules = result.get("applicable_rules", [])
                    
                    if applicable_rules:
                        logger.info(f"âœ… Single text analysis successful. Found rules: {applicable_rules}")
                        
                        # DB ì¡°íšŒ ë° OpenAI ì²˜ë¦¬ (ë™ì¼í•œ ë¡œì§)
                        style_guides_details = await style_guide_service.get_guides_by_applicable_rules(
                            db, rule_ids=applicable_rules
                        )
                        
                        if style_guides_details:
                            # ë¶„ì„ ê²°ê³¼ yield
                            analysis_data = {
                                "applicable_rules": applicable_rules,
                                "style_guide_violations": [
                                    {
                                        "id": f"{g.category}_SG{g.number:03d}",
                                        "category": g.category,
                                        "number": g.number,
                                        "description": ' '.join(g.content) if isinstance(g.content, list) else g.content
                                    }
                                    for g in style_guides_details
                                ],
                                "style_ids": [g.id for g in style_guides_details],
                                "fallback_mode": True,
                                "single_text_info": {
                                    "confidence": result.get("confidence", 0.0),
                                    "adapter_version": result.get("adapter_version", "unknown")
                                }
                            }
                            yield json.dumps({"type": "analysis", "data": analysis_data})
                            
                            # OpenAI êµì •
                            final_openai_prompt = _generate_correction_prompt(style_guides_details, text, prompt)
                            
                            # Fallback ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ë¡œê¹…
                            logger.info("="*80)
                            logger.info("ğŸ“ Fallback Mode OpenAI Prompt:")
                            logger.info(f"Prompt Length: {len(final_openai_prompt)} characters")
                            logger.info(f"Style Guides Count: {len(style_guides_details)}")
                            logger.info("-"*40)
                            logger.info(f"Prompt (first 1500 chars):\n{final_openai_prompt[:1500]}...")
                            if len(final_openai_prompt) > 1500:
                                logger.info(f"... [truncated {len(final_openai_prompt) - 1500} more characters]")
                            logger.info("="*80)
                            # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¤í”„ (í´ë°±ìš©)
                            try:
                                dump_prompt(
                                    "correction_fallback",
                                    final_openai_prompt,
                                    {
                                        "category": category.value if hasattr(category, "value") else str(category),
                                        "style_guides": [
                                            {
                                                "id": g.id,
                                                "number": g.number,
                                                "category": str(getattr(g, "category", ""))
                                            }
                                            for g in style_guides_details
                                        ],
                                        "fallback": True,
                                    },
                                )
                            except Exception:
                                pass

                            async for chunk_str in call_openai_stream_native(final_openai_prompt, ""):
                                try:
                                    delta_chunk = json.loads(chunk_str)
                                    yield json.dumps({"type": "delta", "data": delta_chunk})
                                except json.JSONDecodeError:
                                    pass
                            
                            logger.info("âœ… Fallback processing completed.")
                            return
                            
            except Exception as fallback_error:
                logger.warning(f"Single text fallback also failed: {fallback_error}. Using OpenAI directly.")

    # 2. OpenAI ê¸°ë³¸ êµì • ì‹œë„ (AI ì„œë²„ Flow ì‹¤íŒ¨ ì‹œ Fallback)
    if settings.USE_OPENAI and settings.OPENAI_API_KEY:
        try:
            logger.info("ğŸ¤– Using OpenAI-only correction pipeline as fallback.")
            async for payload in call_openai_correction_stream(prompt, text, category, db):
                yield payload
            return
        except Exception as e:
            logger.warning(f"OpenAI-only pipeline failed: {e}. Will try Gemini if enabled.")

    # 2.5 Gemini êµì • ì‹œë„ (OpenAI ì‹¤íŒ¨ ë˜ëŠ” ë¹„í™œì„± ì‹œ)
    if settings.USE_GEMINI and settings.GEMINI_API_KEY:
        try:
            logger.info("ğŸŸ¢ Using Gemini-only correction pipeline as fallback.")
            from .services.gemini_correction import call_gemini_correction_stream
            async for payload in call_gemini_correction_stream(prompt, text, category, db):
                yield payload
            return
        except Exception as e:
            logger.warning(f"Gemini-only pipeline failed: {e}. Falling back to Mock.")
    
    # 3. Mock ì‘ë‹µ (ìµœì¢… Fallback)
    logger.info("ğŸ“ Using Mock response (all AI services unavailable).")
    mock_response = f"{text}\n\n[This is a mock correction as AI services are unavailable.]"
    for char in mock_response:
        yield json.dumps({"choices": [{"delta": {"content": char}}]})


async def call_ai_correction_stream_openai_only(
    prompt: str,
    text: str,
    category: ArticleCategory | None = None,
    db: AsyncSession | None = None,
):
    """OpenAI APIë§Œì„ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ (í…ŒìŠ¤íŠ¸/í´ë°± ìš©ë„)."""

    if db is None:
        raise ValueError("AsyncSession 'db' is required for OpenAI-only correction stream")

    resolved_category = category or ArticleCategory.BODY

    from .services.openai_correction import call_openai_correction_stream

    async for payload in call_openai_correction_stream(prompt, text, resolved_category, db):
        yield payload


# Backwards compatibility alias (legacy naming)
call_ai_correction_stream_before = call_ai_correction_stream_openai_only

def _parse_metadata_and_clean(text: str) -> tuple[str, list[int]]:
    """
    - í…ìŠ¤íŠ¸ ëë¶€ë¶„ì˜ 'METADATA: {...}' ë¼ì¸ì„ ì°¾ì•„ style_guide_idsë¥¼ ì¶”ì¶œí•˜ê³ ,
      í•´ë‹¹ ë¼ì¸ì„ ì œê±°í•œ ë³¸ë¬¸ì„ ë°˜í™˜.
    - í´ë°±: 'StyleGuide: 1,3,5' íŒ¨í„´ë„ ì§€ì›.
    """
    style_ids: list[int] = []

    # 1) METADATA: JSON ë¼ì¸ ì°¾ê¸° (ë§ˆì§€ë§‰ ì¤„ ìš°ì„ )
    lines = text.splitlines()
    for i in range(len(lines)-1, -1, -1):
        line = lines[i].strip()
        if line.startswith("METADATA:"):
            payload = line[len("METADATA:"):].strip()
            try:
                obj = json.loads(payload)
                ids = obj.get("style_guide_ids", [])
                if isinstance(ids, list):
                    style_ids = [int(x) for x in ids if isinstance(x, int) or (isinstance(x, str) and x.isdigit())]
            except Exception:
                pass
            # ë©”íƒ€ë°ì´í„° ë¼ì¸ì€ ë³¸ë¬¸ì—ì„œ ì œê±°
            lines.pop(i)
            return ("\n".join(lines).rstrip(), style_ids)

    # 2) í´ë°±: "StyleGuide: 1,3,5" íŒ¨í„´
    m = re.search(r"StyleGuide\s*:\s*([\d,\s]+)", text, re.IGNORECASE)
    if m:
        nums = [s.strip() for s in m.group(1).split(",")]
        style_ids = [int(s) for s in nums if s.isdigit()]
        # í•´ë‹¹ íŒ¨í„´ ì œê±°
        cleaned = re.sub(r"StyleGuide\s*:\s*[\d,\s]+", "", text, flags=re.IGNORECASE).rstrip()
        return (cleaned, style_ids)

    return (text, style_ids)

async def _attach_style_guides(db: AsyncSession, *, history_id: int, style_ids: list[int]) -> None:
    """ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ ìŠ¤íƒ€ì¼ê°€ì´ë“œ ì—°ê²° (ë¬¸ì¥ë³„ì´ ì•„ë‹Œ ê²½ìš°)"""
    if not style_ids:
        return
    res = await db.execute(select(StyleGuide.id).where(StyleGuide.id.in_(style_ids)))
    exist_ids = set(res.scalars().all())
    for sid in exist_ids:
        db.add(TextCorrectionHistoryStyle(
            history_id=history_id, 
            style_id=sid,
            sentence_index=-1,  # -1 indicates whole document correction
            note="ì „ì²´ ë¬¸ì„œ êµì •"
        ))


async def next_version_by_news(db: AsyncSession, news_key: str, category: ArticleCategory) -> int:
    """news_key + category ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë²„ì „ ë²ˆí˜¸ ê³„ì‚°"""
    q = select(func.coalesce(func.max(TextCorrectionHistory.version), 0)).where(
        TextCorrectionHistory.news_key==news_key,
        TextCorrectionHistory.category==category
    )
    (maxv,) = (await db.execute(q)).one()
    return int(maxv)+1

async def create_article_with_history(
    db: AsyncSession, *,
    news_key: str,
    category: ArticleCategory,
    user_id: int,
    before_text: str,
    after_text: str,
    prompt: str,
    style_ids: list[int],
    operation_type: OperationType = OperationType.TRANSLATION_CORRECTION,
    source_lang: Optional[str] = None,
    target_lang: Optional[str] = None,
    original_text: Optional[str] = None,
    sentence_corrections: Optional[dict] = None  # ë¬¸ì¥ë³„ êµì • ì •ë³´
) -> TextCorrectionHistory:
    """êµì • ì„±ê³µ í›„ TextCorrectionHistoryë§Œ ìƒì„± """
    
    # 1. news_key + category ê¸°ì¤€ìœ¼ë¡œ version ê³„ì‚°
    version = await next_version_by_news(db, news_key, category)
    
    # 2. TextCorrectionHistory ìƒì„±
    history = TextCorrectionHistory(
        news_key=news_key,
        category=category,
        version=version,     # news_key + category ê¸°ì¤€ ë²„ì „
        original_text=original_text,  # ì™„ì „ ì›ë³¸ í…ìŠ¤íŠ¸ (ë²ˆì—­+êµì •ì‹œ ì›ë³¸ í•œêµ­ì–´)
        before_text=before_text,
        after_text=after_text,
        prompt=prompt,
        operation_type=operation_type,
        source_lang=source_lang,
        target_lang=target_lang,
        created_by_user_id=user_id,
    )
    db.add(history)
    await db.flush()  # history.idë¥¼ ì–»ê¸° ìœ„í•´ flush
    
    # 3. ë¬¸ì¥ë³„ êµì • ì •ë³´ ì €ì¥ (ìˆëŠ” ê²½ìš°)
    if sentence_corrections:
        sentence_count = len(sentence_corrections)
        logger.info(f"Storing sentence-level corrections for {sentence_count} sentences")

        # StyleGuideì˜ number â†’ id ë§¤í•‘ ìƒì„± (í•œë²ˆë§Œ ì¡°íšŒ)
        style_number_to_id = {}
        if style_ids:
            style_guides = await db.execute(
                select(StyleGuide).where(StyleGuide.id.in_(style_ids))
            )
            for sg in style_guides.scalars().all():
                style_number_to_id[sg.number] = sg.id
            logger.debug(f"Loaded {len(style_number_to_id)} style guides for mapping")

        try:
            for idx, sentence_data in enumerate(sentence_corrections):
                # ê° ë¬¸ì¥ì— ëŒ€í•œ ìŠ¤íƒ€ì¼ê°€ì´ë“œë³„ êµì • ì •ë³´ ì €ì¥
                sentence_index = sentence_data.get("sentence_index", idx)  # Default to idx if not provided
                before_text = sentence_data.get("before_text", "")
                after_text = sentence_data.get("after_text", "")
                violations = sentence_data.get("violations", [])
                
                # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
                if sentence_index is None:
                    logger.warning(f"Missing sentence_index for sentence {idx}, using index {idx}")
                    sentence_index = idx
                
                # sentence_indexê°€ ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
                try:
                    sentence_index = int(sentence_index) if sentence_index is not None else idx
                except (ValueError, TypeError):
                    logger.warning(f"Invalid sentence_index type: {type(sentence_index)}, using index {idx}")
                    sentence_index = idx
                
                if not before_text and not after_text:
                    logger.warning(f"Both before_text and after_text are empty for sentence {sentence_index}")
                
                # í•´ë‹¹ ë¬¸ì¥ì— ì ìš©ëœ ìŠ¤íƒ€ì¼ê°€ì´ë“œë“¤ì— ëŒ€í•´ ê°œë³„ ë ˆì½”ë“œ ìƒì„±
                applied_style_ids = set()

                # violationsëŠ” ë‘ ê°€ì§€ í˜•ì‹ì„ ì§€ì›:
                # 1. dict í˜•ì‹: {"style_guide_id": 1, ...} (AI server)
                # 2. string í˜•ì‹: "articles_SG042" (OpenAI)
                for violation in violations:
                    if isinstance(violation, dict):
                        # dict í˜•ì‹: style_guide_id ì§ì ‘ ì‚¬ìš©
                        style_guide_id = violation.get("style_guide_id")
                        if style_guide_id and style_guide_id in style_ids:
                            applied_style_ids.add(style_guide_id)
                    elif isinstance(violation, str):
                        # string í˜•ì‹: rule_idì—ì„œ style_guide_id ì¶”ì¶œ
                        # ì˜ˆ: "articles_SG042" â†’ SG042 â†’ 42ë²ˆ ìŠ¤íƒ€ì¼ê°€ì´ë“œ
                        try:
                            # SG ë’¤ì˜ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: "articles_SG042" â†’ 42)
                            match = re.search(r'SG(\d+)', violation)
                            if match:
                                sg_number = int(match.group(1))
                                # number â†’ id ë§¤í•‘ì—ì„œ style_id ì°¾ê¸°
                                if sg_number in style_number_to_id:
                                    style_id = style_number_to_id[sg_number]
                                    applied_style_ids.add(style_id)
                                    logger.debug(f"Mapped rule_id '{violation}' â†’ style_id {style_id}")
                                else:
                                    logger.warning(f"Style guide SG{sg_number:03d} not found in current style_ids")
                            else:
                                # A/H/C ì½”ë“œ ì§€ì›: A36/H05/C2 ë“± (ê³µë°±/0íŒ¨ë”© í—ˆìš©)
                                match2 = re.match(r'^\s*([AHC])\s*0*(\d+)\s*$', str(violation), re.IGNORECASE)
                                if match2:
                                    sg_number = int(match2.group(2))
                                    if sg_number in style_number_to_id:
                                        style_id = style_number_to_id[sg_number]
                                        applied_style_ids.add(style_id)
                                        logger.debug(f"Mapped rule_code '{violation}' â†’ style_id {style_id}")
                                    else:
                                        logger.warning(f"Style guide number {sg_number} not found in current style_ids")
                        except Exception as e:
                            logger.warning(f"Failed to parse rule_id from violation: {violation}, error: {e}")

                # ìœ„ë°˜ì´ ì—†ëŠ” ë¬¸ì¥ì€ ìŠ¤íƒ€ì¼ê°€ì´ë“œ ë ˆì½”ë“œë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ
                if not applied_style_ids:
                    logger.debug(f"No violations found for sentence {sentence_index}, skipping style guide records")
                    continue  # ë‹¤ìŒ ë¬¸ì¥ìœ¼ë¡œ ê±´ë„ˆë›°ê¸°

                # ê° ì ìš©ëœ ìŠ¤íƒ€ì¼ê°€ì´ë“œì— ëŒ€í•´ ë ˆì½”ë“œ ìƒì„± (ë²Œí¬ ì‚½ì…ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥)
                for style_id in applied_style_ids:
                    try:
                        history_style = TextCorrectionHistoryStyle(
                            history_id=history.id,
                            style_id=style_id,
                            sentence_index=sentence_index,
                            before_text=before_text[:2000] if before_text else "",  # Limit text length
                            after_text=after_text[:2000] if after_text else "",    # Limit text length
                            violations=violations,  # í•´ë‹¹ ë¬¸ì¥ì˜ ëª¨ë“  violations ì €ì¥
                            note=f"ë¬¸ì¥ {sentence_index + 1 if sentence_index is not None else 'N/A'} êµì •"
                        )
                        db.add(history_style)
                        logger.debug(f"Prepared sentence-level record for sentence {sentence_index}, style {style_id}")
                    except Exception as e:
                        logger.error(f"Failed to create sentence-level record for sentence {sentence_index}, style {style_id}: {e}")
                        # Continue with other records even if one fails
                        continue
                        
            logger.info(f"Successfully processed sentence-level corrections for {sentence_count} sentences")
            
        except Exception as e:
            logger.error(f"Error processing sentence corrections: {e}")
            # Fall back to traditional style guide attachment if sentence processing fails
            logger.info("Falling back to traditional style guide attachment")
            await _attach_style_guides(db, history_id=history.id, style_ids=style_ids)
    else:
        # 4. ê¸°ì¡´ ë°©ì‹: ì „ì²´ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìŠ¤íƒ€ì¼ê°€ì´ë“œ ì—°ê²°
        logger.info("No sentence-level corrections provided, using traditional style guide attachment")
        await _attach_style_guides(db, history_id=history.id, style_ids=style_ids)
    
    # 5. ëª¨ë“  ë³€ê²½ì‚¬í•­ ì»¤ë°‹
    await db.commit()
    
    return history

async def save_translation_history(
    db: AsyncSession, *,
    news_key: str,
    category: ArticleCategory,
    user_id: int,
    original_text: str,
    translated_text: str,
    source_lang: str,
    target_lang: str
) -> TextCorrectionHistory:
    """ë²ˆì—­ ì´ë ¥ì„ ì €ì¥ """
    
    # 1. news_key + category ê¸°ì¤€ìœ¼ë¡œ version ê³„ì‚°
    version = await next_version_by_news(db, news_key, category)
    
    # 2. TextCorrectionHistory ìƒì„±
    history = TextCorrectionHistory(
        news_key=news_key,
        category=category,
        version=version,     # news_key + category ê¸°ì¤€ ë²„ì „
        original_text=original_text,  # ì™„ì „ ì›ë³¸ í…ìŠ¤íŠ¸
        before_text=original_text,     # ë²ˆì—­ì˜ ê²½ìš° before_textë„ ì›ë³¸ê³¼ ê°™ìŒ
        after_text=translated_text,
        prompt=None,  # ë²ˆì—­ì—ëŠ” í”„ë¡¬í”„íŠ¸ ì—†ìŒ
        operation_type=OperationType.TRANSLATION,
        source_lang=source_lang,
        target_lang=target_lang,
        created_by_user_id=user_id,
    )
    db.add(history)
    
    # 3. ì»¤ë°‹
    await db.commit()
    await db.refresh(history)
    
    return history

async def list_history(
    db: AsyncSession, *, 
    news_key: str, 
    category: ArticleCategory,
    operation_type: Optional[OperationType] = None
) -> list[TextCorrectionHistory]:
    query = (
        select(TextCorrectionHistory)
        .options(
            selectinload(TextCorrectionHistory.applied_styles)
            .selectinload(TextCorrectionHistoryStyle.style_guide)
        )
        .where(
            TextCorrectionHistory.news_key==news_key, 
            TextCorrectionHistory.category==category
        )
    )
    
    if operation_type:
        query = query.where(TextCorrectionHistory.operation_type==operation_type)
    
    query = query.order_by(TextCorrectionHistory.version.desc())
    
    res = await db.execute(query)
    return res.scalars().all()

async def list_news_history(
    db: AsyncSession, *,
    news_key: str,
    category: Optional[ArticleCategory] = None,  # Accept ArticleCategory enum
    operation_type: Optional[OperationType] = None
) -> list:
    """ë‰´ìŠ¤ íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§ ê°€ëŠ¥, SEO í¬í•¨)

    Args:
        category: Noneì´ë©´ ëª¨ë“  ì¹´í…Œê³ ë¦¬ (articles_translator í¬í•¨), íŠ¹ì • enumì´ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë§Œ
    """

    # Handle SEO category separately
    if category == ArticleCategory.SEO:
        from .models import SEOGenerationHistory
        import json

        query = (
            select(SEOGenerationHistory)
            .where(SEOGenerationHistory.news_key == news_key)
            .order_by(SEOGenerationHistory.created_at.desc())
        )

        res = await db.execute(query)
        seo_histories = res.scalars().all()

        # Convert SEO histories to similar format as TextCorrectionHistory
        result = []
        for idx, history in enumerate(seo_histories):
            # Parse seo_titles from JSON string
            try:
                seo_titles = json.loads(history.seo_titles) if history.seo_titles else []
            except json.JSONDecodeError:
                seo_titles = []

            # Create a mock TextCorrectionHistory-like object for compatibility
            result.append({
                "id": history.id,
                "news_key": history.news_key,
                "category": "SEO",
                "version": len(seo_histories) - idx,  # Reverse index as version
                "before_text": history.input_text,
                "after_text": history.edited_title,
                "operation_type": "CORRECTION",
                "created_at": history.created_at,
                "applied_styles": [],  # SEO doesn't have style guides
                "seo_titles": seo_titles,  # Additional field for SEO
                "raw_response": history.raw_response
            })

        return result

    # Handle regular categories (includes articles_translator)
    query = (
        select(TextCorrectionHistory)
        .options(
            selectinload(TextCorrectionHistory.applied_styles)
            .selectinload(TextCorrectionHistoryStyle.style_guide)
        )
        .where(TextCorrectionHistory.news_key==news_key)
    )

    # category í•„í„° ì¶”ê°€ (Noneì´ë©´ ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬í•¨, articles_translator í¬í•¨)
    if category is not None:
        query = query.where(TextCorrectionHistory.category == category)
    
    if operation_type:
        query = query.where(TextCorrectionHistory.operation_type==operation_type)

    # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    query = query.order_by(TextCorrectionHistory.created_at.desc())

    res = await db.execute(query)
    return res.scalars().all()

async def gpt_generate_title(
    db: AsyncSession,
    input_text: str,
    selected_type: str,
    data_type: str,
    model: str,
    guideline_text: str = None,
    news_key: str = None,
    user_id: int = None
) -> dict:
    """
    GPTë¥¼ ì‚¬ìš©í•˜ì—¬ SEO ìµœì í™”ëœ ì œëª©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        input_text: ì›ë³¸ ì œëª© í…ìŠ¤íŠ¸
        selected_type: ì„ íƒëœ ìœ í˜• (í˜„ì¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        data_type: í—¤ë“œë¼ì¸ ì‘ì„± ê·œì¹™
        model: ì‚¬ìš©í•  GPT ëª¨ë¸ëª…
        guideline_text: ì¶”ê°€ ê°€ì´ë“œë¼ì¸ í…ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    
    Returns:
        ìƒì„±ëœ ì œëª©ë“¤ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ (edited_title, seo_titles, raw_response)
    """
    logger.info("Starting GPT title generation")
    
    # ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê°€ì ¸ì˜¤ê¸°
    from datetime import datetime
    today_date = datetime.now().strftime('%Y-%m-%d')
    logger.info(f"Today's date: {today_date}")
    
    prompt = f"""
    You are an SEO and editorial expert. Your tasks are as follows:
    
    Write a headline within 15 words according to the headline writing rules, and write three additional SEO-optimized headlines that include popular keywords while maintaining readability and relevance to the content. Please write according to the format below. If a Korean title is entered, please write it in English. Do not include any other descriptions or phrases.
    
    Note : Today's date is as follows, and if you need to create an title using today's date, please refer to the following date and create the title.
    
    #Input Title : "{input_text}"
    
    Edited Title:
    
    SEO Title 1:
    
    SEO Title 2:
    
    SEO Title 3:
    
    #Today's date : {today_date}
    
    #Headline writing rules : {data_type}
    """

    dump_prompt(
        "seo",
        prompt,
        {
            "news_key": news_key,
            "model": model,
            "data_type": data_type,
            "user_id": user_id,
        },
    )

    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        from openai import AsyncOpenAI
        client = AsyncOpenAI(api_key=settings.OPENAI_API_SEO_KEY)
        
        input_messages = [
            {
                "role": "system",
                "content": [{"type": "input_text", "text": "You are an SEO and editorial expert."}],
            }
        ]

        if guideline_text:
            input_messages.append(
                {
                    "role": "system",
                    "content": [{"type": "input_text", "text": f"ì¶”ê°€ì ì¸ ì§€ì¹¨: {guideline_text}"}],
                }
            )

        input_messages.append(
            {
                "role": "user",
                "content": [{"type": "input_text", "text": prompt}],
            }
        )

        completion = await client.responses.create(
            model=model,
            input=input_messages,
            temperature=1,
        )

        result = ""
        for item in completion.output or []:
            for content in getattr(item, "content", []) or []:
                if getattr(content, "type", "") == "output_text":
                    result += content.text
        logger.info(f"GPT title generation result: {result[:200]}...")  # ì²˜ìŒ 200ìë§Œ ë¡œê·¸
        
        # Parse the result to extract titles
        lines = result.strip().split('\n')
        edited_title = ""
        seo_titles = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('Edited Title:'):
                edited_title = line.replace('Edited Title:', '').strip()
            elif line.startswith('SEO Title 1:'):
                seo_titles.append(line.replace('SEO Title 1:', '').strip())
            elif line.startswith('SEO Title 2:'):
                seo_titles.append(line.replace('SEO Title 2:', '').strip())
            elif line.startswith('SEO Title 3:'):
                seo_titles.append(line.replace('SEO Title 3:', '').strip())
        
        # Save to SEOGenerationHistory
        if db:
            from .models import SEOGenerationHistory
            import json
            
            history = SEOGenerationHistory(
                news_key=news_key,
                input_text=input_text,
                edited_title=edited_title or result,  # Fallback to full result if parsing failed
                seo_titles=json.dumps(seo_titles),  # Store as JSON string
                raw_response=result,
                prompt_used=prompt,
                model=model,
                data_type=data_type,
                guideline_text=guideline_text,
                created_by_user_id=user_id
            )
            db.add(history)
            await db.commit()
            logger.info(f"Saved SEO generation history with ID: {history.id}")
        
        return {
            "edited_title": edited_title or result,
            "seo_titles": seo_titles,
            "raw_response": result
        }
    
    except Exception as e:
        error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return {
            "edited_title": error_msg,
            "seo_titles": [],
            "raw_response": error_msg
        }


# CMS ì—°ë™ì„ ìœ„í•œ ì„œë¹„ìŠ¤ í•¨ìˆ˜ë“¤
async def save_cms_article(
    db: AsyncSession,
    news_key: str,
    category: ArticleCategory,
    content: str,
    author_name: str
) -> Article:
    """CMSì—ì„œ ì „ì†¡í•œ ë‹¨ì¼ Article ì €ì¥/ìˆ˜ì • (upsert)
    
    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        news_key: ë‰´ìŠ¤ í‚¤
        category: ì¹´í…Œê³ ë¦¬
        content: ì½˜í…ì¸ 
        author_name: CMS ì‘ì„±ì ì´ë¦„
        
    Returns:
        ìƒì„±/ìˆ˜ì •ëœ Article
    """
    import json

    # ì…ë ¥ categoryê°€ ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°(ê´€ë¦¬ì/ì™¸ë¶€ ì—°ë™) ì•ˆì „í•˜ê²Œ Enumìœ¼ë¡œ ë³´ì •
    if isinstance(category, str):
        raw = category.strip().upper().replace("-", "_")
        alias = {
            "SEO": ArticleCategory.SEO,
            "SEO_TITLE": ArticleCategory.SEO,
            "TITLE": ArticleCategory.TITLE,
            "HEADLINE": ArticleCategory.TITLE,
            "HEADLINES": ArticleCategory.TITLE,
            "BODY": ArticleCategory.BODY,
            "ARTICLE": ArticleCategory.BODY,
            "ARTICLES": ArticleCategory.BODY,
            "ARTICLE_TRANSLATOR": ArticleCategory.BODY,
            "ARTICLES_TRANSLATOR": ArticleCategory.BODY,
            "CAPTION": ArticleCategory.CAPTION,
            "CAPTIONS": ArticleCategory.CAPTION,
        }
        category = alias.get(raw, ArticleCategory.BODY)
    
    # Caption ì²˜ë¦¬: ì—¬ëŸ¬ ê°œì˜ captionì„ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
    if category == ArticleCategory.CAPTION:
        try:
            # contentê°€ ì´ë¯¸ JSON í˜•ì‹ì¸ì§€ í™•ì¸
            caption_data = json.loads(content)
            # JSONì´ì§€ë§Œ ì˜¬ë°”ë¥¸ êµ¬ì¡°ê°€ ì•„ë‹Œ ê²½ìš° ì¬êµ¬ì„±
            if not isinstance(caption_data, dict) or "captions" not in caption_data:
                if isinstance(caption_data, list):
                    # ë°°ì—´ì´ë©´ captionsë¡œ ê°ì‹¸ê¸°
                    caption_data = {"captions": caption_data}
                else:
                    # ê·¸ ì™¸ì˜ ê²½ìš° ë‹¨ì¼ captionìœ¼ë¡œ ì²˜ë¦¬
                    caption_data = {"captions": [{"index": 0, "text": str(caption_data)}]}
        except (json.JSONDecodeError, ValueError):
            # JSONì´ ì•„ë‹ˆë©´ ë‹¨ì¼ captionìœ¼ë¡œ ì²˜ë¦¬
            # êµ¬ë¶„ìê°€ ìˆëŠ”ì§€ í™•ì¸ (ì˜ˆ: |||)
            if "|||" in content:
                captions = content.split("|||")
                caption_data = {
                    "captions": [
                        {"index": i, "text": cap.strip()} 
                        for i, cap in enumerate(captions) if cap.strip()
                    ]
                }
            else:
                caption_data = {"captions": [{"index": 0, "text": content}]}
        
        content = json.dumps(caption_data, ensure_ascii=False)
        logger.info(f"Processed caption data for news_key={news_key}: {len(caption_data.get('captions', []))} captions")
    
    # CMS ì‹œìŠ¤í…œ ì‚¬ìš©ì ID (ê³ ì •ê°’)
    CMS_SYSTEM_USER_ID = 1
    
    # ê¸°ì¡´ article ì¡°íšŒ (news_key + category ì¡°í•©ìœ¼ë¡œ)
    result = await db.execute(
        select(Article).where(
            and_(
                Article.news_key == news_key,
                Article.category == category
            )
        )
    )
    article = result.scalar_one_or_none()
    
    if article:
        # ì—…ë°ì´íŠ¸
        article.text = content
        article.cms_author = author_name
        article.updated_at = func.now()
        logger.info(f"Updated CMS article: {news_key}-{category} by {author_name}")
    else:
        # ìƒì„±
        article = Article(
            news_key=news_key,
            category=category,
            text=content,
            user_id=CMS_SYSTEM_USER_ID,  # ê³ ì •ëœ ì‹œìŠ¤í…œ ì‚¬ìš©ì ID
            cms_author=author_name,
            status=ArticleStatus.DRAFT
        )
        db.add(article)
        logger.info(f"Created new CMS article: {news_key}-{category} by {author_name}")
    
    await db.commit()
    await db.refresh(article)
    return article


async def get_article_by_public_id(
    db: AsyncSession,
    public_id: str
) -> Article:
    """Article public_id(UUID)ë¡œ ë‹¨ì¼ Article ì¡°íšŒ"""
    result = await db.execute(
        select(Article).where(Article.public_id == public_id)
    )
    article = result.scalar_one_or_none()
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    return article


async def get_article_by_news_key_and_category(
    db: AsyncSession,
    news_key: str,
    category: ArticleCategory
) -> Article:
    """news_keyì™€ categoryë¡œ Article ì¡°íšŒ"""
    result = await db.execute(
        select(Article).where(
            Article.news_key == news_key,
            Article.category == category
        )
    )
    article = result.scalar_one_or_none()
    if not article:
        raise HTTPException(status_code=404, detail=f"Article not found for news_key={news_key}, category={category}")
    return article


async def get_article_by_identifier(
    db: AsyncSession,
    identifier: str,
    category: Optional[ArticleCategory] = None
) -> Article:
    """identifierê°€ news_keyì¸ì§€ public_idì¸ì§€ ìë™ íŒë‹¨í•˜ì—¬ ì¡°íšŒ
    
    Args:
        identifier: news_key ë˜ëŠ” public_id
        category: categoryê°€ ì œê³µë˜ë©´ news_keyë¡œ ê°„ì£¼í•˜ì—¬ ì¡°íšŒ
    """
    # categoryê°€ ì œê³µë˜ë©´ news_key + categoryë¡œ ì¡°íšŒ
    if category:
        return await get_article_by_news_key_and_category(db, identifier, category)
    
    # UUID í˜•ì‹ì´ë©´ public_idë¡œ ì¡°íšŒ
    try:
        import uuid
        uuid.UUID(identifier)
        return await get_article_by_public_id(db, identifier)
    except ValueError:
        # UUIDê°€ ì•„ë‹ˆë©´ news_keyë¡œ ê°„ì£¼ (ì²« ë²ˆì§¸ ë§¤ì¹­ ë°˜í™˜)
        result = await db.execute(
            select(Article).where(Article.news_key == identifier).limit(1)
        )
        article = result.scalar_one_or_none()
        if not article:
            raise HTTPException(status_code=404, detail=f"Article not found for identifier={identifier}")
        return article


def parse_caption_content(content: str) -> dict:
    """Caption ì½˜í…ì¸ ë¥¼ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„° ë°˜í™˜
    
    Args:
        content: Caption í…ìŠ¤íŠ¸ (JSON ë˜ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸)
    
    Returns:
        dict: {"captions": [{"index": 0, "text": "..."}]}
    """
    import json
    
    try:
        # JSON í˜•ì‹ì¸ ê²½ìš°
        caption_data = json.loads(content)
        if isinstance(caption_data, dict) and "captions" in caption_data:
            return caption_data
        elif isinstance(caption_data, list):
            return {"captions": caption_data}
        else:
            return {"captions": [{"index": 0, "text": str(caption_data)}]}
    except (json.JSONDecodeError, ValueError):
        # JSONì´ ì•„ë‹Œ ê²½ìš°
        if "|||" in content:
            # êµ¬ë¶„ìë¡œ ë¶„ë¦¬
            captions = content.split("|||")
            return {
                "captions": [
                    {"index": i, "text": cap.strip()}
                    for i, cap in enumerate(captions) if cap.strip()
                ]
            }
        else:
            # ë‹¨ì¼ caption
            return {"captions": [{"index": 0, "text": content}]}


async def get_all_captions_for_news(
    db: AsyncSession,
    news_key: str
) -> List[dict]:
    """íŠ¹ì • news_keyì˜ ëª¨ë“  caption ì¡°íšŒ
    
    Returns:
        List[dict]: Caption ëª©ë¡
    """
    try:
        article = await get_article_by_news_key_and_category(
            db, news_key, ArticleCategory.CAPTION
        )
        caption_data = parse_caption_content(article.text)
        return caption_data.get("captions", [])
    except HTTPException:
        # Captionì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return []

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ (í•„ìš”ì‹œ)
async def get_article_by_id(
    db: AsyncSession,
    article_id: int
) -> Article:
    """Article IDë¡œ ë‹¨ì¼ Article ì¡°íšŒ (ë‚´ë¶€ìš©)"""
    result = await db.execute(
        select(Article).where(Article.id == article_id)
    )
    article = result.scalar_one_or_none()
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    return article
