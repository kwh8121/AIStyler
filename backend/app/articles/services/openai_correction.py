# backend/app/articles/services/openai_correction.py
"""
OpenAI API ì§ì ‘ ì—°ë™ êµì • ëª¨ë“ˆ
AI ì„œë²„ ì—†ì´ OpenAI APIë§Œìœ¼ë¡œ ìŠ¤íƒ€ì¼ê°€ì´ë“œ ë¶„ì„ ë° êµì • ìˆ˜í–‰
"""

import json
import time
import logging
import pysbd
import uuid
from pathlib import Path
from datetime import datetime
from typing import AsyncGenerator, List, Dict, Optional
from sqlalchemy.ext.asyncio import AsyncSession

from ...config import settings
from ...styleguides import service as style_guide_service
from ...styleguides.models import StyleCategory
from ..models import ArticleCategory, OperationType
from .translation import translate_text
from .prompt_builder import (
    generate_openai_style_analysis_prompt,
    generate_openai_correction_prompt,
)
import re

logger = logging.getLogger(__name__)


def dump_prompt(kind: str, prompt_text: str, metadata: Optional[Dict] = None) -> None:
    """Persist OpenAI prompts to disk when debugging is enabled."""
    if not settings.OPENAI_DUMP_PROMPTS:
        return

    try:
        base_dir = Path(settings.OPENAI_PROMPT_DUMP_DIR or "logs/openai_prompts")
        base_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
        filename = f"{timestamp}_{kind}_{uuid.uuid4().hex[:8]}.txt"
        file_path = base_dir / filename

        with file_path.open("w", encoding="utf-8") as handle:
            if metadata:
                handle.write(json.dumps(metadata, ensure_ascii=False))
                handle.write("\n\n")
            handle.write(prompt_text)

        logger.debug("Saved OpenAI prompt dump: %s", file_path)
    except Exception as exc:
        logger.debug("Failed to dump OpenAI prompt (%s): %s", kind, exc)


def map_category_to_json(category: ArticleCategory) -> str:
    """ArticleCategory enumì„ JSON ì¹´í…Œê³ ë¦¬ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    mapping = {
        ArticleCategory.SEO: "headlines",
        ArticleCategory.TITLE: "headlines",
        ArticleCategory.BODY: "articles",
        ArticleCategory.CAPTION: "captions",
    }
    return mapping.get(category, "articles")


async def analyze_style_violations_openai(
    text: str,
    category: ArticleCategory,
    style_guides: List,
    db: AsyncSession
) -> Dict:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íƒ€ì¼ê°€ì´ë“œ ìœ„ë°˜ ë¶„ì„

    Returns:
        {
            "violations": [...],
            "total_violations": n,
            "applicable_rules": ["articles_SG001", ...],
            "style_ids": [1, 2, 3, ...]
        }
    """
    try:
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)

        # ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        analysis_prompt = generate_openai_style_analysis_prompt(style_guides, category.value)
        dump_prompt(
            "analysis",
            analysis_prompt,
            {
                "category": category.value,
                "style_guide_count": len(style_guides),
                "text_length": len(text),
            },
        )

        logger.info(f"ğŸ” Analyzing style violations with OpenAI for {len(text)} chars")

        # OpenAI API í˜¸ì¶œ (JSON ëª¨ë“œ)
        analysis_model = settings.OPENAI_MODEL or "gpt-4o-mini"

        response = await client.responses.create(
            model=analysis_model,
            input=[
                {
                    "role": "system",
                    "content": [{"type": "input_text", "text": analysis_prompt}],
                },
                {
                    "role": "user",
                    "content": [{"type": "input_text", "text": text}],
                },
            ],
        )

        # ì‘ë‹µ íŒŒì‹±
        result_text = ""
        for item in response.output or []:
            for content in getattr(item, "content", []) or []:
                if getattr(content, "type", "") == "output_text":
                    result_text += content.text
        logger.debug(f"OpenAI analysis response: {result_text[:500]}...")

        try:
            analysis_result = json.loads(result_text)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse OpenAI response as JSON: {e}")
            analysis_result = {"violations": [], "total_violations": 0}

        # ì ìš© ê°€ëŠ¥í•œ ê·œì¹™ ID ì¶”ì¶œ
        violations = analysis_result.get("violations", [])
        applicable_rules = list(set(v.get("rule_id", "") for v in violations if v.get("rule_id")))

        # DBì—ì„œ style_ids ë§¤í•‘
        style_ids = []
        for guide in style_guides:
            for rule_id in applicable_rules:
                if f"SG{guide.number:03d}" in rule_id:
                    style_ids.append(guide.id)
                    break

        logger.info(f"âœ… Found {len(violations)} violations across {len(applicable_rules)} rules")

        return {
            "violations": violations,
            "total_violations": len(violations),
            "applicable_rules": applicable_rules,
            "style_ids": style_ids
        }

    except Exception as e:
        logger.error(f"OpenAI style analysis failed: {e}")
        return {
            "violations": [],
            "total_violations": 0,
            "applicable_rules": [],
            "style_ids": []
        }


async def call_openai_correction_stream(
    prompt: str,
    text: str,
    category: ArticleCategory,
    db: AsyncSession
) -> AsyncGenerator[str, None]:
    """
    OpenAI APIë§Œì„ ì‚¬ìš©í•œ êµì • ìŠ¤íŠ¸ë¦¬ë°
    1. OpenAIë¡œ ë²ˆì—­
    2. OpenAIë¡œ ìŠ¤íƒ€ì¼ê°€ì´ë“œ ë¶„ì„
    3. OpenAIë¡œ êµì • ìŠ¤íŠ¸ë¦¬ë°
    """

    start_time = time.time()
    logger.info(f"ğŸš€ Starting OpenAI-only correction stream for category={category}")

    try:
        # Step 1: ë²ˆì—­ (í™˜ê²½ ì„¤ì •ì— ë”°ë¥¸ ì œê³µì ì‚¬ìš©)
        logger.info("Step 1: Translating text...")
        yield json.dumps({"status": "translating", "message": "ë²ˆì—­ì¤‘..."})

        translation_start = time.time()
        # ìë™ ê°ì§€ë¡œ ì†ŒìŠ¤ ì–¸ì–´ë¥¼ íŒë³„í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì¬ë²ˆì—­/ì˜¤ë²ˆì—­ì„ ë°©ì§€
        before_en, source_lang, target_lang = await translate_text(
            text,
            source_lang=None,
            target_lang="EN-US"
        )
        translation_time = time.time() - translation_start
        logger.info(f"Translation completed in {translation_time:.3f}s")

        # ì²˜ë¦¬ ì‹œê°„ í¬í•¨í•˜ì—¬ ì „ì†¡
        yield json.dumps({"status": "translation_complete", "message": "ë²ˆì—­ ì™„ë£Œ", "elapsed": round(translation_time, 3)})

        # Step 2: DBì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë“  ìŠ¤íƒ€ì¼ê°€ì´ë“œ ë¡œë“œ
        logger.info(f"Step 2: Loading style guides for category {category.value}")

        # ArticleCategoryë¥¼ StyleCategory enumìœ¼ë¡œ ë³€í™˜ (SEO/Translator í¬í•¨)
        category_map = {
            ArticleCategory.TITLE: StyleCategory.TITLE,
            ArticleCategory.SEO: StyleCategory.TITLE,
            ArticleCategory.BODY: StyleCategory.BODY,
            ArticleCategory.CAPTION: StyleCategory.CAPTION,
        }
        style_category = category_map.get(category, StyleCategory.BODY)

        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ìŠ¤íƒ€ì¼ê°€ì´ë“œ ì¡°íšŒ
        style_guides = await style_guide_service.list_styleguides(
            db,
            category=style_category,
            limit=100
        )

        if not style_guides:
            logger.warning(f"No style guides found for category {style_category}")
            # ìŠ¤íƒ€ì¼ê°€ì´ë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
            for char in before_en:
                yield json.dumps({"choices": [{"delta": {"content": char}}]})
            return

        logger.info(f"Loaded {len(style_guides)} style guides for {style_category}")

        # Step 3: OpenAIë¡œ ìŠ¤íƒ€ì¼ê°€ì´ë“œ ìœ„ë°˜ ë¶„ì„
        logger.info("Step 3: Analyzing style violations with OpenAI...")
        yield json.dumps({"status": "applying_style", "message": "ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ì ìš©ì¤‘..."})

        analysis_start = time.time()
        analysis_result = await analyze_style_violations_openai(
            before_en,
            category,
            style_guides,
            db
        )
        analysis_time = time.time() - analysis_start
        logger.info(f"Analysis completed in {analysis_time:.3f}s")

        # ë¶„ì„ ê²°ê³¼ ì „ì†¡
        violations = analysis_result.get("violations", [])
        applicable_rules = analysis_result.get("applicable_rules", [])
        style_ids = analysis_result.get("style_ids", [])

        yield json.dumps({
            "type": "analysis",
            "data": {
                "applicable_rules": applicable_rules,
                "style_guide_violations": [
                    {
                        "id": rule_id,
                        "description": f"Style guide violation: {rule_id}"
                    }
                    for rule_id in applicable_rules
                ],
                "style_ids": style_ids,
                "violations_count": len(violations)
            }
        })

        yield json.dumps({"status": "analysis_complete", "message": "ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ë¶„ì„ ì™„ë£Œ"})

        # Step 4: OpenAIë¡œ êµì • ìˆ˜í–‰ (ìŠ¤íŠ¸ë¦¬ë°)
        logger.info("Step 4: Correcting text with OpenAI (streaming)...")

        correction_time = 0  # êµì • ì‹œê°„ ì´ˆê¸°í™”

        # ìœ„ë°˜ ì‚¬í•­ì´ ì—†ê³  ì¶”ê°€ ì§€ì¹¨ë„ ì—†ìœ¼ë©´ ì›ë¬¸ ë°˜í™˜
        if not violations and not prompt:
            logger.info("No violations and no additional instructions, returning original text")
            for char in before_en:
                yield json.dumps({
                    "type": "delta",
                    "data": {"choices": [{"delta": {"content": char}}]}
                })
            full_corrected_text = before_en
        else:
            # ìœ„ë°˜ ì‚¬í•­ì´ ìˆê±°ë‚˜ ì¶”ê°€ ì§€ì¹¨ì´ ìˆìœ¼ë©´ êµì • ìˆ˜í–‰
            if violations:
                logger.info(f"Found {len(violations)} violations, performing correction...")
            if prompt:
                logger.info(f"Additional instructions provided: {prompt[:100]}...")

            correction_prompt = generate_openai_correction_prompt(
                before_en,
                violations,
                style_guides,
                prompt,
            )

            dump_prompt(
                "correction",
                correction_prompt,
                {
                    "category": category.value,
                    "violations": len(violations),
                    "style_ids": style_ids,
                    "text_length": len(before_en),
                },
            )

            correction_start = time.time()
            collected_chunks: list[str] = []

            correction_model = settings.OPENAI_MODEL or "gpt-4o-mini"

            # ì¶”ê°€ ì§€ì¹¨ì„ ì‹œìŠ¤í…œ ë ˆë²¨ì—ë„ ë°˜ì˜í•´ ìš°ì„ ìˆœìœ„ ê°•í™”
            system_text = "You are a professional editor.\nFollow the provided rule examples strictly when rewriting; mirror the 'âœ“ Correct' pattern and prefer minimal edits."
            if prompt and str(prompt).strip():
                system_text += f"\nADDITIONAL INSTRUCTION (apply strictly): {str(prompt).strip()}"

            request_params = {
                "model": correction_model,
                "input": [
                    {
                        "role": "system",
                        "content": [{"type": "input_text", "text": system_text}],
                    },
                    {
                        "role": "user",
                        "content": [{"type": "input_text", "text": correction_prompt}],
                    },
                ],
                "stream": True,
                "temperature": 0.1
            }
            if settings.OPENAI_REASONING_EFFORT:
                request_params["reasoning"] = {
                    "effort": settings.OPENAI_REASONING_EFFORT
                }

            from openai import AsyncOpenAI
            client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)

            stream = await client.responses.create(**request_params)

            async for event in stream:
                event_type = getattr(event, "type", None)
                if event_type == "response.output_text.delta":
                    delta_text = getattr(event, "delta", "") or ""
                    if delta_text:
                        collected_chunks.append(delta_text)
                        yield json.dumps({
                            "type": "delta",
                            "data": {"choices": [{"delta": {"content": delta_text}}]}
                        })
                elif event_type in {"response.completed", "response.done"}:
                    break

            correction_time = time.time() - correction_start
            full_corrected_text = "".join(collected_chunks)
            # Courtesy of ... dedupe and ". /" normalization for captions (safety net)
            try:
                if category == ArticleCategory.CAPTION and full_corrected_text:
                    full_corrected_text = re.sub(r'(Courtesy of\s+[^\.\n]+?)(?:\.?\s+\1\.?)+$', r'\1', full_corrected_text, flags=re.IGNORECASE)
                    full_corrected_text = re.sub(r'\.(\s*)/(\s+)(Courtesy of|Yonhap|AP|Reuters|AFP|Getty Images|EPA|Bloomberg|Korea Times)\b', r' \2\3', full_corrected_text, flags=re.IGNORECASE)
            except Exception:
                pass
            logger.info(
                f"Correction completed in {correction_time:.3f}s, output: {len(full_corrected_text)} chars"
            )

        # Step 5: ë¬¸ì¥ë³„ êµì • ì •ë³´ ìƒì„± (ì„ íƒì )
        logger.info("Step 5: Generating sentence-level corrections...")

        # ì›ë³¸ê³¼ êµì •ëœ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
        seg = pysbd.Segmenter(language="en", clean=False)
        original_sentences = seg.segment(before_en)
        # êµì •ì´ ìˆ˜í–‰ë˜ì—ˆìœ¼ë©´ êµì •ëœ í…ìŠ¤íŠ¸, ì•„ë‹ˆë©´ ì›ë³¸
        corrected_sentences = seg.segment(full_corrected_text)

        # ë¬¸ì¥ë³„ êµì • ì •ë³´ ë§¤í•‘
        sentence_corrections = {}
        for idx in range(min(len(original_sentences), len(corrected_sentences))):
            # í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ ìœ„ë°˜ì‚¬í•­ ì°¾ê¸°
            sentence_violations = [
                v["rule_id"] for v in violations
                if v.get("sentence_index") == idx
            ]

            sentence_corrections[idx] = {
                "original": original_sentences[idx] if idx < len(original_sentences) else "",
                "corrected": corrected_sentences[idx] if idx < len(corrected_sentences) else "",
                "violations": sentence_violations
            }

        # ë¬¸ì¥ë³„ êµì • ì •ë³´ ì „ì†¡
        yield json.dumps({
            "type": "sentence_corrections",
            "data": {
                "sentence_corrections": sentence_corrections,
                "total_sentences": len(original_sentences),
                "corrected_sentences": len(corrected_sentences),
                "full_text": full_corrected_text
            }
        })

        yield json.dumps({"status": "sentence_parsing_complete", "message": "ë¬¸ì¥ë³„ êµì • íŒŒì‹± ì™„ë£Œ"})

        # Step 6: ìµœì¢… ë¶„ì„ ê²°ê³¼ (DB ì €ì¥ìš©)
        total_time = time.time() - start_time
        logger.info(f"Total processing time: {total_time:.3f}s")

        final_analysis = {
            "applicable_rules": applicable_rules,
            "style_ids": style_ids,
            "sentence_corrections": sentence_corrections,
            "full_text": full_corrected_text,
            "translation": {
                "before_text": before_en,
                "source_lang": source_lang,
                "target_lang": target_lang,
            },
            "processing_time": {
                "translation": translation_time,
                "analysis": analysis_time,
                "correction": correction_time if (violations or prompt) else 0,
                "total": total_time
            }
        }

        yield json.dumps({"type": "final_analysis", "data": final_analysis})

        # ì™„ë£Œ ë©”ì‹œì§€
        yield json.dumps({"status": "complete", "message": "êµì • ì™„ë£Œ"})

    except Exception as e:
        logger.error(f"OpenAI correction stream error: {e}")
        yield json.dumps({
            "type": "error",
            "data": {"message": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
        })
        raise
